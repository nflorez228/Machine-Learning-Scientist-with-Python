{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Applying logistic regression and SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# # Create and fit the model\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test features, print the results\n",
    "# pred = knn.predict(X_test)[0]\n",
    "# print(\"Prediction for test example 0:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying logistic regression and SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662921348314607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "print(lr.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.96582894e-01, 2.73866353e-03, 6.78442429e-04]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X[:1])\n",
    "# e = 10^ num\n",
    "# 9.95e-1 = 0.99 = 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Underfitting: model is too simple, low traning accuracy\n",
    "\n",
    "* Overfitting: model is too complex, low test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# digits = datasets.load_digits()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# # Apply logistic regression and print scores\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(X_train, y_train)\n",
    "# print(lr.score(X_train,y_train))\n",
    "# print(lr.score(X_test,y_test))\n",
    "\n",
    "# # Apply SVM and print scores\n",
    "# svm = SVC()\n",
    "# svm.fit(X_train, y_train)\n",
    "# print(svm.score(X_train,y_train))\n",
    "# print(svm.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate logistic regression and train\n",
    "# lr = LogisticRegression()\n",
    "# lr.fit(X, y)\n",
    "\n",
    "# # Predict sentiment for a glowing review\n",
    "# review1 = \"I love all muslims in the world so much\"\n",
    "# review1_features = get_features(review1)\n",
    "# print(\"Review:\", review1)\n",
    "# print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# # Predict sentiment for a poor review\n",
    "# review2 = \"I hate the Israel people\"\n",
    "# review2_features = get_features(review2)\n",
    "# print(\"Review:\", review2)\n",
    "# print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/1.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/2.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/3.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# # Define the classifiers\n",
    "# classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# # Fit the classifiers\n",
    "# for c in classifiers:\n",
    "#     c.fit(X, y)\n",
    "\n",
    "# # Plot the classifiers\n",
    "# plot_4_classifiers(X, y, classifiers)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/4.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers: the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.arange(3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(3,6)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@y # dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/5.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/6.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/7.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a loss function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/8.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/9.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The squared error, summed over training examples\n",
    "# def my_loss(w):\n",
    "#     s = 0\n",
    "#     for i in range(y.size):\n",
    "#         # Get the true and predicted target values for example 'i'\n",
    "#         y_i_true = y[i]\n",
    "#         y_i_pred = w@X[i]\n",
    "#         s = s + (y_i_true - y_i_pred)**2\n",
    "#     return s\n",
    "\n",
    "# # Returns the w that makes my_loss(w) smallest\n",
    "# w_fit = minimize(my_loss, X[0]).x\n",
    "# print(w_fit)\n",
    "\n",
    "# # Compare with scikit-learn's LinearRegression coefficients\n",
    "# lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "# print(lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mathematical functions for logistic and hinge losses\n",
    "# def log_loss(raw_model_output):\n",
    "#    return np.log(1+np.exp(-raw_model_output))\n",
    "# def hinge_loss(raw_model_output):\n",
    "#    return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# # Create a grid of values and plot\n",
    "# grid = np.linspace(-2,2,1000)\n",
    "# plt.plot(grid, log_loss(grid), label='logistic')\n",
    "# plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The logistic loss, summed over training examples\n",
    "# def my_loss(w):\n",
    "#     s = 0\n",
    "#     for i in range(y.size):\n",
    "#         raw_model_output = w@X[i]\n",
    "#         s = s + log_loss(raw_model_output * y[i])\n",
    "#     return s\n",
    "\n",
    "# # Returns the w that makes my_loss(w) smallest\n",
    "# w_fit = minimize(my_loss, X[0]).x\n",
    "# print(w_fit)\n",
    "\n",
    "# # Compare with scikit-learn's LogisticRegression\n",
    "# lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
    "# print(lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/10.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/11.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/12.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/14.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/15.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and validaton errors initialized as empty list\n",
    "# train_errs = list()\n",
    "# valid_errs = list()\n",
    "\n",
    "# # Loop over values of C_value\n",
    "# for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "#     # Create LogisticRegression object and fit\n",
    "#     lr = LogisticRegression(C=C_value)\n",
    "#     lr.fit(X_train, y_train)\n",
    "    \n",
    "#     # Evaluate error rates and append to lists\n",
    "#     train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "#     valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "    \n",
    "# # Plot results\n",
    "# plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "# plt.legend((\"train\", \"validation\"))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/16.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify L1 regularization\n",
    "# lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "# # Instantiate the GridSearchCV object and run the search\n",
    "# searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "# searcher.fit(X_train, y_train)\n",
    "\n",
    "# # Report the best parameters\n",
    "# print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# # Find the number of nonzero coefficients (selected features)\n",
    "# best_lr = searcher.best_estimator_\n",
    "# coefs = best_lr.coef_\n",
    "# print(\"Total number of features:\", coefs.size)\n",
    "# print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the indices of the sorted cofficients\n",
    "# inds_ascending = np.argsort(lr.coef_.flatten()) \n",
    "# inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# # Print the most positive words\n",
    "# print(\"Most positive words: \", end=\"\")\n",
    "# for i in range(5):\n",
    "#     print(vocab[inds_descending[i]], end=\", \")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Print most negative words\n",
    "# print(\"Most negative words: \", end=\"\")\n",
    "# for i in range(5):\n",
    "#     print(vocab[inds_ascending[i]], end=\", \")\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/17.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/18.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the regularization strength , large C\n",
    "# model = LogisticRegression(C=1)\n",
    "\n",
    "# # Fit and plot\n",
    "# model.fit(X,y)\n",
    "# plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# # Predict probabilities on training points\n",
    "# prob = model.predict_proba(X)\n",
    "# print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/19.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the regularization strength, small C\n",
    "# model = LogisticRegression(C=0.1)\n",
    "\n",
    "# # Fit and plot\n",
    "# model.fit(X,y)\n",
    "# plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# # Predict probabilities on training points\n",
    "# prob = model.predict_proba(X)\n",
    "# print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/20.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# lr.fit(X,y)\n",
    "\n",
    "# # Get predicted probabilities\n",
    "# proba = lr.predict_proba(X)\n",
    "\n",
    "# # Sort the example indices by their maximum probability\n",
    "# proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# # Show the most confident (least ambiguous) digit\n",
    "# show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# # Show the least confident (most ambiguous) digit\n",
    "# show_digit(proba_inds[0], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/21.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/22.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-vs-rest logistic regression classifier\n",
    "# lr_ovr = LogisticRegression()\n",
    "# lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "# print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "# print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# # Fit softmax classifier\n",
    "# lr_mn = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# lr_mn.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "# print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))\n",
    "\n",
    "#     OVR training accuracy: 0.9948032665181886\n",
    "#     OVR test accuracy    : 0.9644444444444444\n",
    "#     Softmax training accuracy: 1.0\n",
    "#     Softmax test accuracy    : 0.9688888888888889\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print training accuracies\n",
    "# print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "# print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# # Create the binary classifier (class 1 vs. rest)\n",
    "# lr_class_1 = LogisticRegression(C=100)\n",
    "# lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# # Plot the binary classifier (class 1 vs. rest)\n",
    "# plot_classifier(X_train, y_train==1, lr_class_1)\n",
    "\n",
    "# Softmax     training accuracy: 0.996\n",
    "# One-vs-rest training accuracy: 0.916\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/23.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We'll use SVC instead of LinearSVC from now on\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Create/plot the binary classifier (class 1 vs. rest)\n",
    "# svm_class_1 = SVC()\n",
    "# svm_class_1.fit(X_train, y_train==1)\n",
    "# plot_classifier(X_train, y_train==1, svm_class_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/24.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Support vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a linear SVM\n",
    "# svm = SVC(kernel=\"linear\")\n",
    "# svm.fit(X, y)\n",
    "# plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# # Make a new data set keeping only the support vectors\n",
    "# print(\"Number of original examples\", len(X))\n",
    "# print(\"Number of support vectors\", len(svm.support_))\n",
    "# X_small = X[svm.support_]\n",
    "# y_small = y[svm.support_]\n",
    "\n",
    "# # Train a new SVM using only the support vectors\n",
    "# svm_small = SVC(kernel=\"linear\")\n",
    "# svm_small.fit(X, y)\n",
    "# plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/25.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/26.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/27.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/28.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate an RBF SVM\n",
    "# svm = SVC() # defult kernel='rbf'\n",
    "\n",
    "# # Instantiate the GridSearchCV object and run the search\n",
    "# parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "# searcher = GridSearchCV(svm, parameters)\n",
    "# searcher.fit(X, y)\n",
    "\n",
    "# # Report the best parameters\n",
    "# print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Best CV params {'gamma': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate an RBF SVM\n",
    "# svm = SVC()\n",
    "\n",
    "# # Instantiate the GridSearchCV object and run the search\n",
    "# parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "# searcher = GridSearchCV(svm, parameters)\n",
    "# searcher.fit(X_train, y_train)\n",
    "\n",
    "# # Report the best parameters and the corresponding score\n",
    "# print(\"Best CV params\", searcher.best_params_)\n",
    "# print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# # Report the test accuracy using these best parameters\n",
    "# print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
    "\n",
    "# Best CV params {'C': 10, 'gamma': 0.0001}\n",
    "# Best CV accuracy 0.9955481357818586\n",
    "# Test accuracy of best grid search hypers: 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/29.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/30.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/31.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing logistic regression and SVM (and beyond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/32.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/33.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/34.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](AditionalResources/35.png \"k-means clustering with scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We set random_state=0 for reproducibility \n",
    "# linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# # Instantiate the GridSearchCV object and run the search\n",
    "# parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "#              'loss':['hinge', 'log'], 'penalty':['l1','l2']}\n",
    "# searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "# searcher.fit(X_train, y_train)\n",
    "\n",
    "# # Report the best parameters and the corresponding score\n",
    "# print(\"Best CV params\", searcher.best_params_)\n",
    "# print(\"Best CV accuracy\", searcher.best_score_)\n",
    "# print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
    "\n",
    "# Best CV params {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'L1'}\n",
    "# Best CV accuracy 0.94351630867144\n",
    "# Test accuracy of best grid search hypers: 0.9592592592592593\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
